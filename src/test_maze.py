import os
import shutil
from typing import Optional

import hydra
from omegaconf import DictConfig

import numpy as np

from src import utils
from src.datamodule.maze import sample_maze_trajectories, get_maze_env
from src.datamodule.dataset import get_transforms
from src.models.policies.maze import Dijkstra
from src.utils.eval_metrics import spl
from src.utils.visualization import data_gif

log = utils.get_logger(__name__)


def test_maze(cfg: DictConfig) -> Optional[float]:
    """
    Deploy a policy in a maze environment to compute success rate and SPL.

    Please refer to the class docstring or the yaml configs for model and datamodule arguments.

    Args:
        cfg: Config with all hyperparameters for training
    """
    # Set seed for random number generators in pytorch, numpy and python.random
    if cfg.get("seed"):
        utils.seed_experiment(cfg.seed)

    # Get environment name
    # By convention we used {environment}_{agent} to describe a dataset generated by running
    # {agent} in {environment}. We therefore extract the first part of the string to parse
    # only the environment
    # Use first environment in the list of envs
    environment_name = cfg.environments[0].split('_')[:-1]
    environment_name = '_'.join(environment_name)

    # Provide logging directory for frames if we need to generate a gif
    path = 'temp' if cfg.gif_params.name else False

    # Agent benchmark
    env = get_maze_env(maze_name=environment_name)
    agent_policy = hydra.utils.instantiate(cfg.policy)
    agent_policy.transform = get_transforms(resolution=cfg.resize)
    if hasattr(agent_policy, "is_oracle") and agent_policy.is_oracle:
        agent_policy.setup_sim(env)


    # Run simulation
    success, steps = sample_maze_trajectories(env, agent_policy,
                                              n_trajectories=cfg.test_params.n_trajectories,
                                              max_n_steps=cfg.test_params.max_n_steps,
                                              sample_start=cfg.test_params.sample_start,
                                              score=cfg.gif_params.score,
                                              path=path, save_last=True)

    # Oracle benchmark
    env = get_maze_env(maze_name=environment_name)
    policy = Dijkstra(env)
    _, dij_steps = sample_maze_trajectories(env, policy,
                                            n_trajectories=cfg.test_params.n_trajectories,
                                            max_n_steps=cfg.test_params.max_n_steps,
                                            sample_start=cfg.test_params.sample_start,
                                            score=False,
                                            path=False)

    # Report results
    spl_ = spl(success, steps, dij_steps)
    success_rate = np.mean(success)
    log.info(f"Success rate   : {success_rate:.4f}")
    log.info(f"PL             : {spl_/success_rate if success_rate > 0 else 0.0:.4f}")
    log.info(f"SPL            : {spl_:.4f}")
    if hasattr(agent_policy, 'duration'):
        log.info(f"Plan. freq. (amortized)  : {np.sum(steps)/agent_policy.duration:.2f} Hz")
        log.info(f"Plan. freq. (worst case) : {1/agent_policy.duration_max:.2f} Hz")

    if path:
        # Generate gif
        if not os.path.exists('gifs'):
            os.mkdir('gifs')

        success = success if cfg.gif_params.failures else None

        if success is None or not success.all():
            data_gif(path, os.path.join('gifs', cfg.gif_params.name + '.mp4'),
                     cfg.policy._target_, success=success, duration=cfg.gif_params.duration, make_top_down=False)

        # Delete generated data
        shutil.rmtree(path)

    return spl_
